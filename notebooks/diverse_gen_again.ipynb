{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab88dbc1af24921ad795ebe5bb59b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065740e16d174de88181ffa3ff0a0ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb2738c4126402681d305e7865984fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b41c4970a514422ae91bed24f986dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aafc3679c5b4f47b7f285423ee50c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/331k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30251d1e9f664ef39d9f18ef764cd6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce79f38e3d7048ce99c553b916cdb3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfe9aad16ff4fa1a31e1ae0680ed12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b867b3b01b8462ba4123e54838bc9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3b0cc8031841cea6db12b092fbd7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f20eaec248459cbf70aa76e0c927ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from cot_probing.typing import *\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from beartype import beartype\n",
    "\n",
    "\n",
    "model_id = \"hugging-quants/Meta-Llama-3.1-70B-BNB-NF4-BF16\"\n",
    "# model_id = \"hugging-quants/Meta-Llama-3.1-8B-BNB-NF4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  torch_dtype=torch.bfloat16,\n",
    "  low_cpu_mem_usage=True,\n",
    "  device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_determinism(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot_probing.diverse_combinations import generate_all_combinations\n",
    "\n",
    "all_combinations = generate_all_combinations(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_generate_many(\n",
    "    prompt_toks: list[int],\n",
    "    max_new_tokens: int,\n",
    "    temp: float,\n",
    "    n_gen: int,\n",
    "    seed: int,\n",
    ") -> list[list[int]]:\n",
    "    prompt_len = len(prompt_toks)\n",
    "    # TODO\n",
    "    setup_determinism(seed)\n",
    "    responses_tensor = model.generate(\n",
    "        torch.tensor([prompt_toks]).cuda(),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        tokenizer=tokenizer,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        num_return_sequences=n_gen,\n",
    "        stop_strings=[\"Answer:\"],\n",
    "    )[:, prompt_len:]\n",
    "    ret = []\n",
    "    for response_toks in responses_tensor:\n",
    "        response_toks = response_toks.tolist()\n",
    "        if tokenizer.eos_token_id in response_toks:\n",
    "            response_toks = response_toks[: response_toks.index(tokenizer.eos_token_id)]\n",
    "        ret.append(response_toks)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_tok_id = tokenizer.encode(\" Yes\", add_special_tokens=False)[0]\n",
    "no_tok_id = tokenizer.encode(\" No\", add_special_tokens=False)[0]\n",
    "answer_toks = tokenizer.encode(\"Answer:\", add_special_tokens=False)\n",
    "assert len(answer_toks) == 2\n",
    "\n",
    "\n",
    "def categorize_responses(\n",
    "    prompt_toks: list[int], responses: list[list[int]]\n",
    ") -> dict[str, list[list[int]]]:\n",
    "    ret = {\"yes\": [], \"no\": [], \"other\": []}\n",
    "    for response in responses:\n",
    "        if response[-2:] != answer_toks:\n",
    "            # Last two tokens were not \"Answer:\"\n",
    "            ret[\"other\"].append(response)\n",
    "            continue\n",
    "\n",
    "        full_prompt = prompt_toks + response\n",
    "        logits = model(torch.tensor([full_prompt]).cuda()).logits[0, -1]\n",
    "        yes_logit = logits[yes_tok_id].item()\n",
    "        no_logit = logits[no_tok_id].item()\n",
    "        if yes_logit >= no_logit:\n",
    "            ret[\"yes\"].append(response)\n",
    "        else:\n",
    "            ret[\"no\"].append(response)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_responses_single_question(\n",
    "    combined_prompts, max_new_tokens: int, temp: float, n_gen: int, seed: int\n",
    "):\n",
    "    prompt_unb = combined_prompts[\"unb_yes\"]\n",
    "    prompt_no = combined_prompts[\"no_yes\"]\n",
    "    question = prompt_unb.rsplit(\"Question:\", 1)[-1][1:]\n",
    "    print(\"###\")\n",
    "    print(question)\n",
    "    prompt_toks_unb = tokenizer.encode(prompt_unb)\n",
    "    prompt_toks_no = tokenizer.encode(prompt_no)\n",
    "    resp_unb = hf_generate_many(\n",
    "        prompt_toks_unb,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temp=temp,\n",
    "        n_gen=n_gen,\n",
    "        seed=seed,\n",
    "    )\n",
    "    resp_no = hf_generate_many(\n",
    "        prompt_toks_no,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temp=temp,\n",
    "        n_gen=n_gen,\n",
    "        seed=seed,\n",
    "    )\n",
    "    res = {\n",
    "        \"unb\": categorize_responses(prompt_toks_unb, resp_unb),\n",
    "        \"no\": categorize_responses(prompt_toks_unb, resp_no),\n",
    "    }\n",
    "    for variant in [\"unb\", \"no\"]:\n",
    "        print(f\"{variant=}\")\n",
    "        for key in [\"yes\", \"no\"]:\n",
    "            print(f\"{key} {len(res[variant][key])}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def analyze_responses(all_combinations, max_new_tokens: int, temp: float, n_gen: int):\n",
    "    results = []\n",
    "    for i, combined_prompts in enumerate(all_combinations):\n",
    "        print(f\"{i}\")\n",
    "        res = analyze_responses_single_question(\n",
    "            combined_prompts, max_new_tokens, temp, n_gen\n",
    "        )\n",
    "        results.append(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "###\n",
      "Was Barack Obama's father born in a country where English is the predominant language?\n",
      "Let's think step by step:\n",
      "-\n",
      "variant='unb'\n",
      "yes 0\n",
      "no 10\n",
      "variant='no'\n",
      "yes 0\n",
      "no 10\n",
      "1\n",
      "###\n",
      "Did Fargo recieve more Oscar nominations than The Truman Show?\n",
      "Let's think step by step:\n",
      "-\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_responses \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_responses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_combinations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36manalyze_responses\u001b[0;34m(all_combinations, max_new_tokens, temp, n_gen)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, combined_prompts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_combinations):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_responses_single_question\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombined_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_gen\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36manalyze_responses_single_question\u001b[0;34m(combined_prompts, max_new_tokens, temp, n_gen)\u001b[0m\n\u001b[1;32m     10\u001b[0m prompt_toks_no \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt_no)\n\u001b[1;32m     11\u001b[0m resp_unb \u001b[38;5;241m=\u001b[39m hf_generate_many(\n\u001b[1;32m     12\u001b[0m     prompt_toks_unb,\n\u001b[1;32m     13\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[1;32m     14\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemp,\n\u001b[1;32m     15\u001b[0m     n_gen\u001b[38;5;241m=\u001b[39mn_gen,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m resp_no \u001b[38;5;241m=\u001b[39m \u001b[43mhf_generate_many\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_toks_no\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m res \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munb\u001b[39m\u001b[38;5;124m\"\u001b[39m: categorize_responses(prompt_toks_unb, resp_unb),\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m: categorize_responses(prompt_toks_unb, resp_no),\n\u001b[1;32m     26\u001b[0m }\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variant \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mhf_generate_many\u001b[0;34m(prompt_toks, max_new_tokens, temp, n_gen, seed)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m     10\u001b[0m setup_determinism(seed)\n\u001b[0;32m---> 11\u001b[0m responses_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt_toks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnswer:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[:, prompt_len:]\n\u001b[1;32m     21\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response_toks \u001b[38;5;129;01min\u001b[39;00m responses_tensor:\n",
      "File \u001b[0;32m/workspace/cot-probing/.env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/cot-probing/.env/lib/python3.10/site-packages/transformers/generation/utils.py:2048\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2040\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2041\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2042\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2043\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2045\u001b[0m     )\n\u001b[1;32m   2047\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2048\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2059\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2060\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2061\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2062\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2068\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/cot-probing/.env/lib/python3.10/site-packages/transformers/generation/utils.py:3044\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3042\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3043\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3044\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3046\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_responses = analyze_responses(\n",
    "    all_combinations, max_new_tokens=120, temp=0.9, n_gen=10, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "unb:\n",
      "\n",
      "yes: 1\n",
      " The Amazon River flows eastward\n",
      "- The Congo River also flows eastward\n",
      "Answer:\n",
      "-----\n",
      "\n",
      "no: 9\n",
      " The Amazon River flows eastward towards the Atlantic Ocean\n",
      "- The Congo River flows westward towards the Atlantic Ocean\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward\n",
      "- The Congo River flows westward\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows from west to east\n",
      "- The Congo River flows from south to north\n",
      "- West and east are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward from the Andes Mountains towards the Atlantic Ocean\n",
      "- The Congo River flows northwestward from the Congo Basin towards the Atlantic Ocean\n",
      "- Eastward and northwestward are different directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward\n",
      "- The Congo River flows westward\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward\n",
      "- The Congo River flows westward\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastwards\n",
      "- The Congo River flows westwards\n",
      "- Eastwards and westwards are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows from west to east\n",
      "- The Congo River flows from south to north\n",
      "- West to east and south to north are not the same direction\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward\n",
      "- The Congo River flows westward\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      "###\n",
      "no:\n",
      "\n",
      "yes: 0\n",
      "\n",
      "no: 10\n",
      " The Amazon River flows eastward from the Andes mountains to the Atlantic Ocean\n",
      "- The Congo River flows westward from the Great Rift Valley to the Atlantic Ocean\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward\n",
      "- The Congo River flows westward\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows east to west\n",
      "- The Congo River flows north to south\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward\n",
      "- The Congo River flows westward\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward from the Andes Mountains in Peru towards the Atlantic Ocean\n",
      "- The Congo River flows westward from the East African Rift Valley in Tanzania towards the Atlantic Ocean\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward\n",
      "- The Congo River flows westward\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward from the Andes Mountains to the Atlantic Ocean\n",
      "- The Congo River flows southward from the Democratic Republic of Congo to the Atlantic Ocean\n",
      "- Eastward and southward are different directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward towards the Atlantic Ocean\n",
      "- The Congo River flows westward towards the Atlantic Ocean\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows eastward into the Atlantic Ocean\n",
      "- The Congo River flows westward into the Atlantic Ocean\n",
      "- Eastward and westward are opposite directions\n",
      "Answer:\n",
      "-----\n",
      " The Amazon River flows from west to east\n",
      "- The Congo River flows from east to west\n",
      "Answer:\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# showing the responses\n",
    "responses = all_responses[0]\n",
    "for variant in [\"unb\", \"no\"]:\n",
    "    print(\"###\")\n",
    "    print(f\"{variant}:\")\n",
    "    for key, resp in responses[variant].items():\n",
    "        print()\n",
    "        print(f\"{key}: {len(responses[variant][key])}\")\n",
    "        for resp in responses[variant][key]:\n",
    "            print(tokenizer.decode(resp))\n",
    "            print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run all of it a few times with different seeds (same for generation and same for getting the FSPs),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
