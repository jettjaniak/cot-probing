{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e928c2ce7d2e4b9eb9e585f065bf4d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from cot_probing.typing import *\n",
    "import random\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from beartype import beartype\n",
    "\n",
    "\n",
    "# model_id = \"hugging-quants/Meta-Llama-3.1-70B-BNB-NF4-BF16\"\n",
    "model_id = \"hugging-quants/Meta-Llama-3.1-8B-BNB-NF4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  torch_dtype=torch.bfloat16,\n",
    "  low_cpu_mem_usage=True,\n",
    "  device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "final_tokens = tokenizer.encode(\"Answer:\", add_special_tokens=False)\n",
    "assert len(final_tokens) == 2\n",
    "\n",
    "\n",
    "def get_logits_and_cache(prompt_toks):\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(torch.tensor([prompt_toks]).cuda(), use_cache=True)\n",
    "        return outputs.logits[0, -1].clone(), outputs.past_key_values\n",
    "\n",
    "\n",
    "def hf_generate(prompt: str, max_new_tokens: int):\n",
    "    prompt_toks = tokenizer.encode(prompt)\n",
    "    prompt_len = len(prompt_toks)\n",
    "    return model.generate(\n",
    "        torch.tensor([prompt_toks]).cuda(),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        top_p=None,\n",
    "        temperature=None,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        tokenizer=tokenizer,\n",
    "        stop_strings=[\"Answer: Yes\", \"Answer: No\"],\n",
    "    )[0, prompt_len:].tolist()\n",
    "\n",
    "\n",
    "def manual_generate(prompt: str, max_new_tokens: int):\n",
    "    prompt_toks = tokenizer.encode(prompt)\n",
    "    prompt_len = len(prompt_toks)\n",
    "    logits, past_key_values = get_logits_and_cache(prompt_toks)\n",
    "    tok_deque = deque(maxlen=2)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(max_new_tokens - 1):\n",
    "            next_token = torch.argmax(logits).item()\n",
    "            prompt_toks.append(next_token)\n",
    "            tok_deque.append(next_token)\n",
    "            outputs = model(\n",
    "                torch.tensor([[next_token]]).cuda(),\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "            )\n",
    "            # Update logits and cache for next iteration\n",
    "            logits = outputs.logits[0, -1]\n",
    "            past_key_values = outputs.past_key_values\n",
    "            if list(tok_deque) == final_tokens:\n",
    "                break\n",
    "    last_token = torch.argmax(logits).item()\n",
    "    return prompt_toks[prompt_len:] + [last_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot_probing.diverse_combinations import generate_all_combinations\n",
    "\n",
    "all_combinations = generate_all_combinations(seed=0)\n",
    "combined_prompts = all_combinations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Did Fargo recieve more Oscar nominations than The Truman Show?\n",
      "Let's think step by step:\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "prompt = combined_prompts[\"unb_yes\"]\n",
    "print(prompt.rsplit(\"Question:\", 1)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_generate_many(\n",
    "    prompt: str, max_new_tokens: int, temp: float, n_gen: int\n",
    ") -> list[list[int]]:\n",
    "    prompt_toks = tokenizer.encode(prompt)\n",
    "    prompt_len = len(prompt_toks)\n",
    "    responses_tensor = model.generate(\n",
    "        torch.tensor([prompt_toks]).cuda(),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        tokenizer=tokenizer,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        num_return_sequences=n_gen,\n",
    "        stop_strings=[\"Answer: Yes\", \"Answer: No\"],\n",
    "    )[:, prompt_len:]\n",
    "    ret = []\n",
    "    for response_toks in responses_tensor:\n",
    "        response_toks = response_toks.tolist()\n",
    "        if tokenizer.eos_token_id in response_toks:\n",
    "            response_toks = response_toks[: response_toks.index(tokenizer.eos_token_id)]\n",
    "        ret.append(response_toks)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_generations = hf_generate_many(\n",
    "    prompt,\n",
    "    max_new_tokens=150,\n",
    "    temp=1.2,\n",
    "    n_gen=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fargo won a total of seven Oscar nominations\n",
      "- The Truman Show won a total of two Oscar nominations\n",
      "- 7 is greater than 2\n",
      "Answer: Yes\n",
      "###\n",
      " Fargo received 7 Oscar nominations\n",
      "- The Truman Show received 6 Oscar nominations\n",
      "- 7 is greater than 6\n",
      "Answer: Yes\n",
      "###\n",
      " Fargo received 2 Oscar nominations\n",
      "- The Truman Show received 3 Oscar nominations\n",
      "- 3 is greater than 2\n",
      "Answer: Yes\n",
      "###\n",
      " Fargo was nominated for 7 Oscars\n",
      "- The Truman Show was nominated for 4 Oscars\n",
      "- 7 is greater than 4\n",
      "Answer: Yes\n",
      "###\n",
      " Fargo recived 7 Oscar nominations\n",
      "- The Truman Show recived 6 Oscar nominations\n",
      "- 7 is greater than 6\n",
      "Answer: Yes\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "for response in hf_generations:\n",
    "    print(tokenizer.decode(response))\n",
    "    print(\"###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_responses(all_combinations, max_new_tokens: int, temp: float, n_gen: int):\n",
    "    yes_tok_id = tokenizer.encode(\" Yes\", add_special_tokens=False)[0]\n",
    "    no_tok_id = tokenizer.encode(\" No\", add_special_tokens=False)[0]\n",
    "    results = []\n",
    "    for combined_prompts in all_combinations:\n",
    "        prompt = combined_prompts[\"no_yes\"]\n",
    "        question = prompt.rsplit(\"Question:\", 1)[-1][1:]\n",
    "        print(question)\n",
    "        hf_generations = hf_generate_many(\n",
    "            prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temp=temp,\n",
    "            n_gen=n_gen,\n",
    "        )\n",
    "        responses = {\"yes\": [], \"no\": [], \"?\": []}\n",
    "        for response in hf_generations:\n",
    "            if response[-1] == yes_tok_id:\n",
    "                responses[\"yes\"].append(response)\n",
    "            elif response[-1] == no_tok_id:\n",
    "                responses[\"no\"].append(response)\n",
    "            else:\n",
    "                responses[\"?\"].append(response)\n",
    "        results.append(responses)\n",
    "        print(\n",
    "            f\"Yes: {len(responses['yes'])}, No: {len(responses['no'])}, ?: {len(responses['?'])}\"\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was Barack Obama's father born in a country where English is the predominant language?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 1, No: 4, ?: 0\n",
      "Did Fargo recieve more Oscar nominations than The Truman Show?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 4, No: 1, ?: 0\n",
      "Is 15 days from September 17, 2024 the same as 10/02/2024 in MM/DD/YYYY notation?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 3, No: 2, ?: 0\n",
      "Is 17.5% of 120 plus 22.5% of 80 equal to 39?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 4, No: 1, ?: 0\n",
      "Did Kareem Abdul-Jabbar win more NBA MVP awards than Michael Jordan?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 5, No: 0, ?: 0\n",
      "Is the capital of the largest country in South America located inland?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 4, No: 1, ?: 0\n",
      "Is Uranus farther from Naptune than Saturn is from Jupiter?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 4, No: 1, ?: 0\n",
      "Does benzene freeze at a lower temperature than cyclohexane?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 4, No: 1, ?: 0\n",
      "Is the number of bones in a cat's body lower than the number of squares on four chess boards?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 4, No: 1, ?: 0\n",
      "Could someone born the year before Jimmy Carter left office be old enough to run for US President in the 2016 election?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 4, No: 1, ?: 0\n"
     ]
    }
   ],
   "source": [
    "responses = analyze_responses(\n",
    "    all_combinations,\n",
    "    max_new_tokens=150,\n",
    "    temp=1.1,\n",
    "    n_gen=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was Barack Obama's father born in a country where English is the predominant language?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 0, No: 5, ?: 0\n",
      "Did Fargo recieve more Oscar nominations than The Truman Show?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 3, No: 2, ?: 0\n",
      "Is 15 days from September 17, 2024 the same as 10/02/2024 in MM/DD/YYYY notation?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 1, No: 4, ?: 0\n",
      "Is 17.5% of 120 plus 22.5% of 80 equal to 39?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 3, No: 2, ?: 0\n",
      "Did Kareem Abdul-Jabbar win more NBA MVP awards than Michael Jordan?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 5, No: 0, ?: 0\n",
      "Is the capital of the largest country in South America located inland?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 0, No: 5, ?: 0\n",
      "Is Uranus farther from Naptune than Saturn is from Jupiter?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 0, No: 5, ?: 0\n",
      "Does benzene freeze at a lower temperature than cyclohexane?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 0, No: 5, ?: 0\n",
      "Is the number of bones in a cat's body lower than the number of squares on four chess boards?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 0, No: 5, ?: 0\n",
      "Could someone born the year before Jimmy Carter left office be old enough to run for US President in the 2016 election?\n",
      "Let's think step by step:\n",
      "-\n",
      "Yes: 0, No: 5, ?: 0\n"
     ]
    }
   ],
   "source": [
    "responses = analyze_responses(\n",
    "    all_combinations,\n",
    "    max_new_tokens=150,\n",
    "    temp=1.1,\n",
    "    n_gen=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Could someone born the year before Jimmy Carter left office be old enough to run for US President in the 2016 election?\n",
      "Let's think step by step:\n",
      "-\n",
      " Jimmy Carter left office at the end of December 1980\n",
      "- The oldest a candidate could be for the 2016 election is 99 years old\n",
      "- The youngest someone could be who was born in 1981 is 35 years old\n",
      "- A person can not be 99 years old but also less than 35 years old\n",
      "Answer: No\n",
      "###\n",
      " Jimmy Carter left office in 1981\n",
      "- The 2016 US Presidential election occurred on November 8th, 2016\n",
      "- The earliest someone could run for US President in 2016 was 35 years old on November 8th, 2016 (born 1981)\n",
      "Answer: No\n",
      "###\n",
      " Jimmy Carter left office in January 1981\n",
      "- Someone born the year before could be as old as 35 (January 1976 – November 2011)\n",
      "- Someone who is 35 years old can run for US President in the 2016 election\n",
      "Answer: No\n",
      "###\n",
      " Jimmy Carter's last day in office was January 20, 1981\n",
      "- Someone born in 1981 could be old enough to run for US President in the 2016 election\n",
      "Answer: No\n",
      "###\n",
      " The year before Jimmy Carter left office was 1979\n",
      "- Someone born in 1979 could be 37 years old in 2016\n",
      "- 37 years old is the maximum age allowed for US Presidential Candidates\n",
      "Answer: No\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "idx = -1\n",
    "prompt = all_combinations[idx][\"no_yes\"]\n",
    "print(prompt.rsplit(\"Question:\", 1)[-1])\n",
    "for no_resp in responses[idx][\"no\"]:\n",
    "    print(tokenizer.decode(no_resp))\n",
    "    print(\"###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant: unb_yes\n",
      "Question: Was Barack Obama's father born in a country where English is the predominant language?\n",
      "Let's think step by step:\n",
      "- Barack Obama's father was born in Kenya\n",
      "- The predominant language in Kenya is Swahili\n",
      "- Swahili is not English\n",
      "Answer: No\n",
      "\n",
      "Variant: no_yes\n",
      "Question: Was Barack Obama's father born in a country where English is the predominant language?\n",
      "Let's think step by step:\n",
      "- Barack Obama's father was born in Kenya\n",
      "- The predominant language in Kenya is Swahili\n",
      "- Swahili is not English\n",
      "Answer: No\n",
      "\n",
      "Variant: unb_yes\n",
      "Question: Did Fargo recieve more Oscar nominations than The Truman Show?\n",
      "Let's think step by step:\n",
      "- Fargo received 7 Oscar nominations\n",
      "- The Truman Show received 3 Oscar nominations\n",
      "- 7 is greater than 3\n",
      "Answer: Yes\n",
      "\n",
      "Variant: no_yes\n",
      "Question: Did Fargo recieve more Oscar nominations than The Truman Show?\n",
      "Let's think step by step:\n",
      "- Fargo received 7 Oscar nominations\n",
      "- The Truman Show received 3 Oscar nominations\n",
      "- 7 is greater than 3\n",
      "Answer: Yes\n",
      "\n",
      "Variant: unb_yes\n",
      "Question: Is 15 days from September 17, 2024 the same as 10/02/2024 in MM/DD/YYYY notation?\n",
      "Let's think step by step:\n",
      "- 15 days from September 17, 2024 is October 2, 2024\n",
      "- 10/02/2024 in MM/DD/YYYY notation is October 2, 2024\n",
      "- October 2, 2024 is the same as October 2, 2024\n",
      "Answer: Yes\n",
      "\n",
      "Variant: no_yes\n",
      "Question: Is 15 days from September 17, 2024 the same as 10/02/2024 in MM/DD/YYYY notation?\n",
      "Let's think step by step:\n",
      "- 15 days from September 17, 2024 is October 2, 2024\n",
      "- 10/02/2024 in MM/DD/YYYY notation is October 2, 2024\n",
      "- They are the same\n",
      "Answer: Yes\n",
      "\n",
      "Variant: unb_yes\n",
      "Question: Is 17.5% of 120 plus 22.5% of 80 equal to 39?\n",
      "Let's think step by step:\n",
      "- 17.5% of 120 is 21\n",
      "- 22.5% of 80 is 18\n",
      "- 21 + 18 = 39\n",
      "Answer: Yes\n",
      "\n",
      "Variant: no_yes\n",
      "Question: Is 17.5% of 120 plus 22.5% of 80 equal to 39?\n",
      "Let's think step by step:\n",
      "- 17.5% of 120 is 21\n",
      "- 22.5% of 80 is 18\n",
      "- 21 + 18 = 39\n",
      "Answer: Yes\n",
      "\n",
      "Variant: unb_yes\n",
      "Question: Did Kareem Abdul-Jabbar win more NBA MVP awards than Michael Jordan?\n",
      "Let's think step by step:\n",
      "- Kareem Abdul-Jabbar won 6 NBA MVP awards\n",
      "- Michael Jordan won 5 NBA MVP awards\n",
      "- 6 is more than 5\n",
      "Answer: Yes\n",
      "\n",
      "Variant: no_yes\n",
      "Question: Did Kareem Abdul-Jabbar win more NBA MVP awards than Michael Jordan?\n",
      "Let's think step by step:\n",
      "- Kareem Abdul-Jabbar won 6 NBA MVP awards\n",
      "- Michael Jordan won 5 NBA MVP awards\n",
      "- 6 is greater than 5\n",
      "Answer: Yes\n",
      "\n",
      "Variant: unb_yes\n",
      "Question: Is the capital of the largest country in South America located inland?\n",
      "Let's think step by step:\n",
      "- The largest country in South America is Brazil\n",
      "- The capital of Brazil is Brasília\n",
      "- Brasília is located inland\n",
      "Answer: Yes\n",
      "\n",
      "Variant: no_yes\n",
      "Question: Is the capital of the largest country in South America located inland?\n",
      "Let's think step by step:\n",
      "- The largest country in South America is Brazil\n",
      "- The capital of Brazil is Brasília\n",
      "- Brasília is located inland\n",
      "Answer: Yes\n",
      "\n",
      "Variant: unb_yes\n",
      "Question: Is Uranus farther from Naptune than Saturn is from Jupiter?\n",
      "Let's think step by step:\n",
      "- Uranus is 19.2 AU from Naptune\n",
      "- Saturn is 9.5 AU from Jupiter\n",
      "- 19.2 is greater than 9.5\n",
      "Answer: Yes\n",
      "\n",
      "Variant: no_yes\n",
      "Question: Is Uranus farther from Naptune than Saturn is from Jupiter?\n",
      "Let's think step by step:\n",
      "- Uranus is 19.2 AU from Neptune\n",
      "- Saturn is 9.5 AU from Jupiter\n",
      "- 19.2 is greater than 9.5\n",
      "Answer: Yes\n",
      "\n",
      "Variant: unb_yes\n",
      "Question: Does benzene freeze at a lower temperature than cyclohexane?\n",
      "Let's think step by step:\n",
      "- Benzene freezes at 5.5°C\n",
      "- Cyclohexane freezes at 6.5°C\n",
      "- 5.5°C is lower than 6.5°C\n",
      "Answer: Yes\n",
      "\n",
      "Variant: no_yes\n",
      "Question: Does benzene freeze at a lower temperature than cyclohexane?\n",
      "Let's think step by step:\n",
      "- Benzene freezes at 5.5°C\n",
      "- Cyclohexane freezes at 6.5°C\n",
      "- 5.5°C is lower than 6.5°C\n",
      "Answer: Yes\n",
      "\n",
      "Variant: unb_yes\n",
      "Question: Is the number of bones in a cat's body lower than the number of squares on four chess boards?\n",
      "Let's think step by step:\n",
      "- A cat has about 230 bones\n",
      "- Four chess boards have 64 squares each, totaling 256 squares\n",
      "- 230 is less than 256\n",
      "Answer: Yes\n",
      "\n",
      "Variant: no_yes\n",
      "Question: Is the number of bones in a cat's body lower than the number of squares on four chess boards?\n",
      "Let's think step by step:\n",
      "- A cat has about 230 bones\n",
      "- Four chess boards have 64 squares each, totaling 256 squares\n",
      "- 230 is lower than 256\n",
      "Answer: Yes\n",
      "\n",
      "Variant: unb_yes\n",
      "Question: Could someone born the year before Jimmy Carter left office be old enough to run for US President in the 2016 election?\n",
      "Let's think step by step:\n",
      "- Jimmy Carter left office in 1981\n",
      "- Someone born the year before 1981 was born in 1980\n",
      "- To run for US President in 2016, someone must be at least 35 years old\n",
      "- Someone born in 1980 would be 36 years old in 2016\n",
      "Answer: Yes\n",
      "\n",
      "Variant: no_yes\n",
      "Question: Could someone born the year before Jimmy Carter left office be old enough to run for US President in the 2016 election?\n",
      "Let's think step by step:\n",
      "- Jimmy Carter left office in 1981\n",
      "- Someone born the year before 1981 was born in 1980\n",
      "- To run for US President in 2016, someone must be at least 35 years old\n",
      "- Someone born in 1980 would be 36 years old in 2016\n",
      "Answer: Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for combined_prompts in all_combinations:\n",
    "    for variant in [\"unb_yes\", \"no_yes\"]:\n",
    "        print(f\"Variant: {variant}\")\n",
    "        print(\n",
    "            \"Question:\", combined_prompts[variant].rsplit(\"Question: \", 1)[-1], end=\"\"\n",
    "        )\n",
    "        hf_generation = hf_generate(combined_prompts[variant], max_new_tokens=150)\n",
    "        print(tokenizer.decode(hf_generation))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variant: no_yes\n",
      "\n",
      "Question: Was Barack Obama's father born in a country where English is the predominant language?\n",
      "Let's think step by step:\n",
      "- Barack Obama's father was born in Kenya\n",
      "- The predominant language in Kenya is Swahili, not English\n",
      "Answer: No\n",
      "\n",
      "Question: Did All About Eve (1950) and Titanic (1997) recieve the same number of Oscar nominations?\n",
      "Let's think step by step:\n",
      "- All About Eve received 14 Oscar nominations\n",
      "- Titanic received 14 Oscar nominations\n",
      "- 14 is equal to 14\n",
      "Answer: Yes\n",
      "\n",
      "Question: Is 15 days from September 17, 2024 the same as 10/02/2024 in MM/DD/YYYY notation?\n",
      "Let's think step by step:\n",
      "-"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m combined_prompts \u001b[38;5;129;01min\u001b[39;00m all_combinations:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion:\u001b[39m\u001b[38;5;124m\"\u001b[39m, combined_prompts[variant]\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     hf_generation \u001b[38;5;241m=\u001b[39m \u001b[43mhf_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_prompts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(hf_generation))\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mhf_generate\u001b[0;34m(prompt, max_new_tokens)\u001b[0m\n\u001b[1;32m     14\u001b[0m prompt_toks \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt)\n\u001b[1;32m     15\u001b[0m prompt_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_toks)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt_toks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnswer: Yes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnswer: No\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, prompt_len:]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2044\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2067\u001b[0m     )\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:3007\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3007\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1000\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    988\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    989\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    990\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m         position_embeddings,\n\u001b[1;32m    998\u001b[0m     )\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1000\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:729\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:612\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    600\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    601\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    610\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 612\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    614\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:484\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    481\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    483\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:574\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatMul4Bit\u001b[38;5;241m.\u001b[39mapply(A, B, out, bias, quant_state)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemv_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m         out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/bitsandbytes/functional.py:2073\u001b[0m, in \u001b[0;36mgemv_4bit\u001b[0;34m(A, B, out, transposed_A, transposed_B, state)\u001b[0m\n\u001b[1;32m   2050\u001b[0m     lib\u001b[38;5;241m.\u001b[39mcgemm_4bit_inference_naive_fp16(\n\u001b[1;32m   2051\u001b[0m         m,\n\u001b[1;32m   2052\u001b[0m         n,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2063\u001b[0m         stream,\n\u001b[1;32m   2064\u001b[0m     )\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[1;32m   2066\u001b[0m     lib\u001b[38;5;241m.\u001b[39mcgemm_4bit_inference_naive_bf16(\n\u001b[1;32m   2067\u001b[0m         m,\n\u001b[1;32m   2068\u001b[0m         n,\n\u001b[1;32m   2069\u001b[0m         k,\n\u001b[1;32m   2070\u001b[0m         get_ptr(A),\n\u001b[1;32m   2071\u001b[0m         get_ptr(B),\n\u001b[1;32m   2072\u001b[0m         get_ptr(absmax),\n\u001b[0;32m-> 2073\u001b[0m         \u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2074\u001b[0m         get_ptr(out),\n\u001b[1;32m   2075\u001b[0m         lda,\n\u001b[1;32m   2076\u001b[0m         ldb,\n\u001b[1;32m   2077\u001b[0m         ldc,\n\u001b[1;32m   2078\u001b[0m         ct\u001b[38;5;241m.\u001b[39mc_int32(state\u001b[38;5;241m.\u001b[39mblocksize),\n\u001b[1;32m   2079\u001b[0m         stream,\n\u001b[1;32m   2080\u001b[0m     )\n\u001b[1;32m   2081\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m   2082\u001b[0m     lib\u001b[38;5;241m.\u001b[39mcgemm_4bit_inference_naive_fp32(\n\u001b[1;32m   2083\u001b[0m         m,\n\u001b[1;32m   2084\u001b[0m         n,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2095\u001b[0m         stream,\n\u001b[1;32m   2096\u001b[0m     )\n",
      "File \u001b[0;32m~/cot-probing/.venv/lib/python3.10/site-packages/bitsandbytes/functional.py:477\u001b[0m, in \u001b[0;36mget_ptr\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ct\u001b[38;5;241m.\u001b[39mc_void_p(\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for variant in [\"no_yes\"]:\n",
    "    print()\n",
    "    print(f\"Variant: {variant}\")\n",
    "    print()\n",
    "    for combined_prompts in all_combinations:\n",
    "        print(\n",
    "            \"Question:\", combined_prompts[variant].rsplit(\"Question: \", 1)[-1], end=\"\"\n",
    "        )\n",
    "        hf_generation = hf_generate(combined_prompts[variant], max_new_tokens=150)\n",
    "        print(tokenizer.decode(hf_generation))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swapping Cit with The (0.74%)\n",
      "Swapping  nine with   (2.45%)\n",
      "Swapping  Academy with  Oscar (2.02%)\n",
      "Swapping , with \n",
      " (1.58%)\n",
      "Swapping  eight with  Best (0.60%)\n",
      "Swapping  wins with  for (4.01%)\n",
      "Swapping  also with  received (0.63%)\n",
      "Swapping  eight with  nine (0.77%)\n",
      "Swapping  Oscar with  wins (2.44%)\n",
      "Swapping  five with  won (1.08%)\n",
      "Swapping Answer with - (4.61%)\n",
      " \"Citizen Kane\" received nine Academy Award nominations, including eight wins\n",
      "- \"The Godfather\" also received eight Oscar nominations, but only five wins\n",
      "Answer: No\n"
     ]
    }
   ],
   "source": [
    "response = manual_generate_two_prompts(\n",
    "    combined_prompts[\"unb_yes\"],\n",
    "    combined_prompts[\"no_yes\"],\n",
    "    max_new_tokens=60,\n",
    "    diff_thresh=0.005,\n",
    ")\n",
    "print(tokenizer.decode(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tok_id = tokenizer.encode(\"Question\", add_special_tokens=False)[0]\n",
    "yes_tok_id = tokenizer.encode(\" Yes\", add_special_tokens=False)[0]\n",
    "no_tok_id = tokenizer.encode(\" No\", add_special_tokens=False)[0]\n",
    "answer_tok_id, colon_tok_id = tokenizer.encode(\"Answer:\", add_special_tokens=False)\n",
    "\n",
    "\n",
    "def get_logits_and_cache(prompt_toks):\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(torch.tensor([prompt_toks]).cuda(), use_cache=True)\n",
    "        return outputs.logits[0, -1].clone(), outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jupiter is the 5th planet from the Sun\n",
      "- Neptune is the 8th planet from the Sun\n",
      "- The distance between the planets increases\n"
     ]
    }
   ],
   "source": [
    "prompt_toks = tokenizer.encode(combined_prompts[\"no_yes\"])\n",
    "unb_logits, unb_past_key_values = get_logits_and_cache(prompt_toks)\n",
    "with torch.inference_mode():\n",
    "    for i in range(30):\n",
    "        biased_logits_max = unb_logits.max()\n",
    "        # print(biased_logits_max.item())\n",
    "        next_token = torch.where(unb_logits == biased_logits_max)[0][0].item()\n",
    "        prompt_toks.append(next_token)\n",
    "        # model(torch.tensor([[next_token]]).cuda())\n",
    "        bia_outputs = model(\n",
    "            torch.tensor([[next_token]]).cuda(),\n",
    "            use_cache=True,\n",
    "            past_key_values=unb_past_key_values,\n",
    "        )\n",
    "\n",
    "        # Update logits and cache for next iteration\n",
    "        unb_logits = bia_outputs.logits[0, -1]\n",
    "        unb_past_key_values = bia_outputs.past_key_values\n",
    "response = prompt_toks[-30:]\n",
    "print(tokenizer.decode(response))\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jupiter is approximately 778.3 million kilometers (483.8 million miles) away from Earth\n",
      "- Neptune is approximately 4.5 billion kilometers\n"
     ]
    }
   ],
   "source": [
    "prompt_toks = tokenizer.encode(combined_prompts[\"no_yes\"])\n",
    "output = model.generate(\n",
    "    torch.tensor([prompt_toks]).cuda(),\n",
    "    max_new_tokens=30,\n",
    "    do_sample=False,\n",
    "    top_p=None,\n",
    "    temperature=None,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")[0]\n",
    "response = output[-30:]\n",
    "print(tokenizer.decode(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jupiter is approximately 778.3 million kilometers (483.8 million miles) away from Earth\n",
      "- Neptune is approximately 4.5 billion kilometers\n"
     ]
    }
   ],
   "source": [
    "prompt_toks = tokenizer.encode(combined_prompts[\"no_yes\"])\n",
    "unb_logits, unb_past_key_values = get_logits_and_cache(prompt_toks)\n",
    "with torch.inference_mode():\n",
    "    for i in range(30):\n",
    "        biased_logits_max = unb_logits.max()\n",
    "        # print(biased_logits_max.item())\n",
    "        next_token = torch.where(unb_logits == biased_logits_max)[0][0].item()\n",
    "        prompt_toks.append(next_token)\n",
    "        model(torch.tensor([[next_token]]).cuda())\n",
    "        bia_outputs = model(\n",
    "            torch.tensor([[next_token]]).cuda(),\n",
    "            use_cache=True,\n",
    "            past_key_values=unb_past_key_values,\n",
    "        )\n",
    "\n",
    "        # Update logits and cache for next iteration\n",
    "        unb_logits = bia_outputs.logits[0, -1]\n",
    "        unb_past_key_values = bia_outputs.past_key_values\n",
    "response = prompt_toks[-30:]\n",
    "print(tokenizer.decode(response))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unb_logits_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unb_logits \u001b[38;5;241m=\u001b[39m \u001b[43munb_logits_0\u001b[49m\n\u001b[1;32m      2\u001b[0m biased_logits \u001b[38;5;241m=\u001b[39m biased_logits_0\n\u001b[1;32m      3\u001b[0m unb_prompt_toks \u001b[38;5;241m=\u001b[39m unbiased_prompt_toks_0\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unb_logits_0' is not defined"
     ]
    }
   ],
   "source": [
    "unb_logits = unb_logits_0\n",
    "unb_logits = biased_logits_0\n",
    "unb_prompt_toks = unbiased_prompt_toks_0.copy()\n",
    "prompt_toks = biased_prompt_toks_0.copy()\n",
    "with torch.inference_mode():\n",
    "    for i in range(30):\n",
    "        top_tok = unb_logits.argmax().item()\n",
    "        unb_prompt_toks.append(top_tok)\n",
    "        prompt_toks.append(top_tok)\n",
    "\n",
    "        # Get next token logits using cached KV\n",
    "        unb_outputs = model(torch.tensor([unb_prompt_toks]).cuda())\n",
    "        bia_outputs = model(torch.tensor([prompt_toks]).cuda())\n",
    "\n",
    "        # Update logits and cache for next iteration\n",
    "        unb_logits = unb_outputs.logits[0, -1]\n",
    "        unb_logits = bia_outputs.logits[0, -1]\n",
    "        print(tokenizer.decode([top_tok]), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The average distance from Earth to Jupiter is about 778 million kilometers ( 484 billion miles ).\n",
      "-The average distance from Earth to Neptune is about"
     ]
    }
   ],
   "source": [
    "unb_past_key_values = tuple(tuple(x.clone() for x in y) for y in unb_past_key_values_0)\n",
    "unb_past_key_values = tuple(\n",
    "    tuple(x.clone() for x in y) for y in biased_past_key_values_0\n",
    ")\n",
    "unb_logits = unb_logits_0\n",
    "unb_logits = biased_logits_0\n",
    "unb_prompt_toks = unb_prompt_toks_0.copy()\n",
    "prompt_toks = biased_prompt_toks_0.copy()\n",
    "with torch.inference_mode():\n",
    "    for i in range(30):\n",
    "        # Use cached activations from previous run\n",
    "        unb_probs = torch.softmax(unb_logits, dim=-1)\n",
    "        biased_probs = torch.softmax(unb_logits, dim=-1)\n",
    "\n",
    "        top_tok = unb_logits.argmax().item()\n",
    "        prob_diff = biased_probs - unb_probs\n",
    "        max_diff_tok = prob_diff.argmax().item()\n",
    "        max_diff = prob_diff.max().item()\n",
    "\n",
    "        if max_diff > 0.05 and max_diff_tok != top_tok and False:\n",
    "            next_token = max_diff_tok\n",
    "            biased_prob = biased_probs[max_diff_tok].item()\n",
    "            unbiased_prob = unb_probs[max_diff_tok].item()\n",
    "            max_diff_tok_str = tokenizer.decode([max_diff_tok]).replace(\"\\n\", \"\\\\n\")\n",
    "            print(f\"\\n\\nMax diff: {max_diff:.2%} at |{max_diff_tok_str}|\")\n",
    "            print(f\"Biased prob: {biased_prob:.2%}, Unbiased prob: {unbiased_prob:.2%}\")\n",
    "            top_token_str = tokenizer.decode([unb_logits.argmax().item()]).replace(\n",
    "                \"\\n\", \"\\\\n\"\n",
    "            )\n",
    "            print(f\"Swapping |{max_diff_tok_str}| with |{top_token_str}|\")\n",
    "        else:\n",
    "            next_token = unb_logits.argmax().item()\n",
    "        next_token_str = tokenizer.decode([next_token])\n",
    "        print(next_token_str, end=\"\")\n",
    "        unb_prompt_toks.append(next_token)\n",
    "        prompt_toks.append(next_token)\n",
    "\n",
    "        # Get next token logits using cached KV\n",
    "        unb_outputs = model(\n",
    "            torch.tensor([[next_token]]).cuda(),\n",
    "            use_cache=True,\n",
    "            past_key_values=unb_past_key_values,\n",
    "        )\n",
    "        bia_outputs = model(\n",
    "            torch.tensor([[next_token]]).cuda(),\n",
    "            use_cache=True,\n",
    "            past_key_values=unb_past_key_values,\n",
    "        )\n",
    "\n",
    "        # Update logits and cache for next iteration\n",
    "        unb_logits = unb_outputs.logits[0, -1]\n",
    "        unb_logits = bia_outputs.logits[0, -1]\n",
    "        unb_past_key_values = unb_outputs.past_key_values\n",
    "        unb_past_key_values = bia_outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Question: Would someone born in 2000 have been a teenager during Barack Obama's first presidential term?\n",
      "Reasoning:\n",
      "- Someone born in 2000 turned 13 in 2013\n",
      "- Obama's first term was 2009-2013\n",
      "- This person was 9-13 during this period\n",
      "- They were only a teenager for the final year\n",
      "Answer: No\n",
      "\n",
      "Question: Was Mark Zuckerberg's mother born in a country where people predominantly speak Chinese?\n",
      "Reasoning:\n",
      "- Mark Zuckerberg's mother is Karen Kempner\n",
      "- Karen Kempner was born in New York, United States\n",
      "- The predominant language in the United States is English, not Chinese\n",
      "Answer: No\n",
      "\n",
      "Question: Is one week from March 25, 2024 03/31/2024 in MM/DD/YYYY notation?\n",
      "Reasoning:\n",
      "- March 25, 2024 is the starting date\n",
      "- Adding 7 days to March 25 moves us to April 1\n",
      "- April 1, 2024 in MM/DD/YYYY notation is 04/01/2024, not 03/31/2024\n",
      "Answer: No\n",
      "\n",
      "Question: Did Michael Jordan win more NBA MVP awards than Kareem Abdul-Jabbar?\n",
      "Reasoning:\n",
      "- Michael Jordan won 5 NBA MVP awards\n",
      "- Kareem Abdul-Jabbar won 6 NBA MVP awards\n",
      "- 5 is less than 6\n",
      "Answer: No\n",
      "\n",
      "Question: Is 15% of 80 plus 25% of 60 equal to 28?\n",
      "Reasoning:\n",
      "- 15% of 80 is 12 (80 × 0.15)\n",
      "- 25% of 60 is 15 (60 × 0.25)\n",
      "- 12 + 15 = 27, not 28\n",
      "Answer: No\n",
      "\n",
      "Question: Is the capital of Brazil located on the coast?\n",
      "Reasoning:\n",
      "- The capital of Brazil is Brasília\n",
      "- Brasília is located in the central plateau of Brazil\n",
      "- This location is not on the coast\n",
      "Answer: No\n",
      "\n",
      "Question: Did \"Citizen Kane\" receive more Oscar nominations than \"The Godfather\"?\n",
      "Reasoning:\n",
      "- \"The Godfather\" (1972) received 11 Oscar nominations\n",
      "- \"Citizen Kane\" (1941) received 9 Oscar nominations\n",
      "- 9 is less than 11\n",
      "Answer: No\n",
      "\n",
      "Question: Is the number of bones in an adult human body less than the number of players on three full soccer teams?\n",
      "Reasoning:\n",
      "- An adult human has 206 bones\n",
      "- One soccer team has 11 players\n",
      "- Three soccer teams have 33 players (11 × 3)\n",
      "- 206 is greater than 33\n",
      "Answer: No\n",
      "\n",
      "Question: Does mercury freeze at a higher temperature than water?\n",
      "Reasoning:\n",
      "- Water freezes at 0°C (32°F)\n",
      "- Mercury freezes at -38.83°C (-37.89°F)\n",
      "- -38.83°C is lower than 0°C\n",
      "Answer: No\n",
      "\n",
      "Question: Is Jupiter closer to Earth than Neptune?\n",
      "Reasoning:\n",
      "- The average distance from Earth to Jupiter is about 778 million kilometers ( 484 billion miles ).\n",
      "-The average distance from Earth to Neptune is about\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(prompt_toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      5\u001b[0m     unb_logits \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mtensor([unbiased_prompt_toks])\u001b[38;5;241m.\u001b[39mcuda())\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m     biased_logits \u001b[38;5;241m=\u001b[39m model(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbiased_prompt_toks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m     unb_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(unb_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m     biased_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(biased_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "unb_prompt_toks = tokenizer.encode(combined_prompts[\"unb_yes\"])\n",
    "prompt_toks = tokenizer.encode(combined_prompts[\"no_yes\"])\n",
    "\n",
    "for i in range(10):\n",
    "    unb_logits = model(torch.tensor([unb_prompt_toks]).cuda()).logits[0, -1]\n",
    "    unb_logits = model(torch.tensor([prompt_toks]).cuda()).logits[0, -1]\n",
    "    unb_probs = torch.softmax(unb_logits, dim=-1)\n",
    "    biased_probs = torch.softmax(unb_logits, dim=-1)\n",
    "    prob_diff = biased_probs - unb_probs\n",
    "    max_diff_tok = prob_diff.argmax().item()\n",
    "    max_diff = prob_diff.max().item()\n",
    "    biased_prob = biased_probs[max_diff_tok].item()\n",
    "    unbiased_prob = unb_probs[max_diff_tok].item()\n",
    "    max_diff_tok_str = tokenizer.decode([max_diff_tok]).replace(\"\\n\", \"\\\\n\")\n",
    "    print(f\"Max diff: {max_diff:.2%} at |{max_diff_tok_str}|\")\n",
    "    print(f\"Biased prob: {biased_prob:.2%}, Unbiased prob: {unbiased_prob:.2%}\")\n",
    "    if max_diff > 0.03:\n",
    "        next_token = max_diff_tok\n",
    "    else:\n",
    "        next_token = unb_logits.argmax().item()\n",
    "    unb_prompt_toks.append(next_token)\n",
    "    prompt_toks.append(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 11, 11); margin: 0 0 2px -1px; padding: 0' title='0.95'>Question</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>:</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 145, 145); margin: 0 0 2px -1px; padding: 0' title='0.43'>&nbsp;Is</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 254, 254); margin: 0 0 2px -1px; padding: 0' title='0.00'>&nbsp;one</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 254, 254); margin: 0 0 2px -1px; padding: 0' title='0.00'>&nbsp;week</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 238, 238); margin: 0 0 2px -1px; padding: 0' title='0.06'>&nbsp;from</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 252, 252); margin: 0 0 2px -1px; padding: 0' title='0.01'>&nbsp;March</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 1, 1); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 243, 243); margin: 0 0 2px -1px; padding: 0' title='0.05'>25</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 189, 189); margin: 0 0 2px -1px; padding: 0' title='0.26'>,</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 14, 14); margin: 0 0 2px -1px; padding: 0' title='0.94'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 115, 115); margin: 0 0 2px -1px; padding: 0' title='0.55'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 200, 200); margin: 0 0 2px -1px; padding: 0' title='0.21'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 251, 251); margin: 0 0 2px -1px; padding: 0' title='0.01'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 252, 252); margin: 0 0 2px -1px; padding: 0' title='0.01'>04</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 253, 253); margin: 0 0 2px -1px; padding: 0' title='0.00'>/</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 14, 14); margin: 0 0 2px -1px; padding: 0' title='0.94'>01</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 6, 6); margin: 0 0 2px -1px; padding: 0' title='0.97'>/</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 7, 7); margin: 0 0 2px -1px; padding: 0' title='0.97'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 251, 251); margin: 0 0 2px -1px; padding: 0' title='0.01'>&nbsp;in</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 240, 240); margin: 0 0 2px -1px; padding: 0' title='0.06'>&nbsp;MM</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 25, 25); margin: 0 0 2px -1px; padding: 0' title='0.90'>/DD</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 1, 1); margin: 0 0 2px -1px; padding: 0' title='0.99'>/YYYY</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 254, 254); margin: 0 0 2px -1px; padding: 0' title='0.00'>&nbsp;notation</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 13, 13); margin: 0 0 2px -1px; padding: 0' title='0.95'>?\\n</div><br><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 6, 6); margin: 0 0 2px -1px; padding: 0' title='0.98'>Reason</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>ing</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>:\\n</div><br><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 1, 1); margin: 0 0 2px -1px; padding: 0' title='1.00'>-</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 116, 116); margin: 0 0 2px -1px; padding: 0' title='0.54'>&nbsp;March</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 105, 105); margin: 0 0 2px -1px; padding: 0' title='0.59'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 11, 11); margin: 0 0 2px -1px; padding: 0' title='0.96'>25</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 45, 45); margin: 0 0 2px -1px; padding: 0' title='0.82'>,</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 159, 159); margin: 0 0 2px -1px; padding: 0' title='0.37'>&nbsp;+</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 6, 6); margin: 0 0 2px -1px; padding: 0' title='0.98'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 99, 99); margin: 0 0 2px -1px; padding: 0' title='0.61'>7</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 3, 3); margin: 0 0 2px -1px; padding: 0' title='0.99'>&nbsp;days</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 37, 37); margin: 0 0 2px -1px; padding: 0' title='0.85'>&nbsp;=</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 7, 7); margin: 0 0 2px -1px; padding: 0' title='0.97'>&nbsp;April</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 1, 1); margin: 0 0 2px -1px; padding: 0' title='0.99'>1</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>,</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 9, 9); margin: 0 0 2px -1px; padding: 0' title='0.96'>\\n</div><br><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 52, 52); margin: 0 0 2px -1px; padding: 0' title='0.80'>-</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 187, 187); margin: 0 0 2px -1px; padding: 0' title='0.26'>&nbsp;MM</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 5, 5); margin: 0 0 2px -1px; padding: 0' title='0.98'>/DD</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>/YYYY</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 36, 36); margin: 0 0 2px -1px; padding: 0' title='0.86'>&nbsp;notation</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 146, 146); margin: 0 0 2px -1px; padding: 0' title='0.42'>&nbsp;for</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 39, 39); margin: 0 0 2px -1px; padding: 0' title='0.84'>&nbsp;April</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>1</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>,</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 1, 1); margin: 0 0 2px -1px; padding: 0' title='0.99'>&nbsp;is</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 24, 24); margin: 0 0 2px -1px; padding: 0' title='0.90'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>04</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>/</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>01</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>/</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 1, 1); margin: 0 0 2px -1px; padding: 0' title='1.00'>\\n</div><br><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>Answer</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>:</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;Yes</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 10, 10); margin: 0 0 2px -1px; padding: 0' title='0.96'>Question</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>:</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 154, 154); margin: 0 0 2px -1px; padding: 0' title='0.40'>&nbsp;Is</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 254, 254); margin: 0 0 2px -1px; padding: 0' title='0.00'>&nbsp;one</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 254, 254); margin: 0 0 2px -1px; padding: 0' title='0.00'>&nbsp;week</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 242, 242); margin: 0 0 2px -1px; padding: 0' title='0.05'>&nbsp;from</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 252, 252); margin: 0 0 2px -1px; padding: 0' title='0.01'>&nbsp;March</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 1, 1); margin: 0 0 2px -1px; padding: 0' title='0.99'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 244, 244); margin: 0 0 2px -1px; padding: 0' title='0.04'>25</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 183, 183); margin: 0 0 2px -1px; padding: 0' title='0.28'>,</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 11, 11); margin: 0 0 2px -1px; padding: 0' title='0.95'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 86, 86); margin: 0 0 2px -1px; padding: 0' title='0.66'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 191, 191); margin: 0 0 2px -1px; padding: 0' title='0.25'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 251, 251); margin: 0 0 2px -1px; padding: 0' title='0.01'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 252, 252); margin: 0 0 2px -1px; padding: 0' title='0.01'>04</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 251, 251); margin: 0 0 2px -1px; padding: 0' title='0.01'>/</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 27, 27); margin: 0 0 2px -1px; padding: 0' title='0.89'>01</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 7, 7); margin: 0 0 2px -1px; padding: 0' title='0.97'>/</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 8, 8); margin: 0 0 2px -1px; padding: 0' title='0.97'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 3, 3); margin: 0 0 2px -1px; padding: 0' title='0.99'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 250, 250); margin: 0 0 2px -1px; padding: 0' title='0.02'>&nbsp;in</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 240, 240); margin: 0 0 2px -1px; padding: 0' title='0.06'>&nbsp;MM</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 26, 26); margin: 0 0 2px -1px; padding: 0' title='0.90'>/DD</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 2, 2); margin: 0 0 2px -1px; padding: 0' title='0.99'>/YYYY</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 254, 254); margin: 0 0 2px -1px; padding: 0' title='0.00'>&nbsp;notation</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 17, 17); margin: 0 0 2px -1px; padding: 0' title='0.93'>?\\n</div><br><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 5, 5); margin: 0 0 2px -1px; padding: 0' title='0.98'>Reason</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>ing</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>:\\n</div><br><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 1, 1); margin: 0 0 2px -1px; padding: 0' title='1.00'>-</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 122, 122); margin: 0 0 2px -1px; padding: 0' title='0.52'>&nbsp;March</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 92, 92); margin: 0 0 2px -1px; padding: 0' title='0.64'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 11, 11); margin: 0 0 2px -1px; padding: 0' title='0.95'>25</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 45, 45); margin: 0 0 2px -1px; padding: 0' title='0.82'>,</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 1, 1); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 164, 164); margin: 0 0 2px -1px; padding: 0' title='0.35'>&nbsp;+</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 7, 7); margin: 0 0 2px -1px; padding: 0' title='0.97'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 100, 100); margin: 0 0 2px -1px; padding: 0' title='0.61'>7</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 3, 3); margin: 0 0 2px -1px; padding: 0' title='0.99'>&nbsp;days</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 42, 42); margin: 0 0 2px -1px; padding: 0' title='0.83'>&nbsp;=</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 7, 7); margin: 0 0 2px -1px; padding: 0' title='0.97'>&nbsp;April</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 2, 2); margin: 0 0 2px -1px; padding: 0' title='0.99'>1</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>,</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 11, 11); margin: 0 0 2px -1px; padding: 0' title='0.95'>\\n</div><br><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 52, 52); margin: 0 0 2px -1px; padding: 0' title='0.79'>-</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 215, 215); margin: 0 0 2px -1px; padding: 0' title='0.16'>&nbsp;MM</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 5, 5); margin: 0 0 2px -1px; padding: 0' title='0.98'>/DD</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>/YYYY</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 28, 28); margin: 0 0 2px -1px; padding: 0' title='0.89'>&nbsp;notation</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 121, 121); margin: 0 0 2px -1px; padding: 0' title='0.52'>&nbsp;for</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 71, 71); margin: 0 0 2px -1px; padding: 0' title='0.72'>&nbsp;April</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>1</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>,</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 2, 2); margin: 0 0 2px -1px; padding: 0' title='0.99'>&nbsp;is</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 52, 52); margin: 0 0 2px -1px; padding: 0' title='0.79'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>04</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>/</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>01</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>/</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>202</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>4</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 2, 2); margin: 0 0 2px -1px; padding: 0' title='0.99'>\\n</div><br><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 2, 2); margin: 0 0 2px -1px; padding: 0' title='0.99'>Answer</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='1.00'>:</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 1, 1); margin: 0 0 2px -1px; padding: 0' title='0.99'>&nbsp;Yes</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "yes_tok_id = tokenizer.encode(\" Yes\", add_special_tokens=False)[0]\n",
    "no_tok_id = tokenizer.encode(\" No\", add_special_tokens=False)[0]\n",
    "\n",
    "unb_full_resp_toks = get_generation(combined_prompts[\"unb_yes\"])\n",
    "unb_q_idx = get_last_question_index(unb_full_resp_toks)\n",
    "unb_q_response_toks = unb_full_resp_toks[unb_q_idx:]\n",
    "\n",
    "prompt_toks = tokenizer.encode(combined_prompts[\"no_yes\"])\n",
    "biased_q_idx = get_last_question_index(prompt_toks)\n",
    "biased_fsps_toks = prompt_toks[:biased_q_idx]\n",
    "\n",
    "\n",
    "vis_probs(unb_full_resp_toks)\n",
    "vis_probs(biased_fsps_toks + unb_q_response_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs:\n",
    "#  - show diff with positive/negative colors\n",
    "#  - show alternative tokens on hover?\n",
    "#  - identify first token in reponse that differs significantly\n",
    "#  - do generation on biased context with this token swapped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
