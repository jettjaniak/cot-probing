{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot_probing.typing import *\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def general_patching_hook_fn(\n",
    "    module, input, output, pos: slice | int, resid: Float[torch.Tensor, \" seq model\"]\n",
    "):\n",
    "    # output is 2 el. tuple\n",
    "    output = output[0]\n",
    "    # we're running batch size 1\n",
    "    output = output[0]\n",
    "    # shape is (seq, model)\n",
    "    output[pos] = resid\n",
    "\n",
    "\n",
    "def layer_to_hook_point(layer: int):\n",
    "    if layer == 0:\n",
    "        return \"model.embed_tokens\"\n",
    "    return f\"model.layers.{layer-1}\"\n",
    "\n",
    "\n",
    "def hook_point_to_layer(hook_point: str):\n",
    "    if hook_point == \"model.embed_tokens\":\n",
    "        return 0\n",
    "    return int(hook_point.split(\".\")[-1]) + 1\n",
    "\n",
    "\n",
    "def patched_run(\n",
    "    model: PreTrainedModel,\n",
    "    input_ids: list[int],\n",
    "    resid_by_pos_by_layer: dict[\n",
    "        int, dict[slice | int, Float[torch.Tensor, \" _seq model\"]]\n",
    "    ],\n",
    "):\n",
    "    hooks = []\n",
    "    hook_points = set(layer_to_hook_point(i) for i in resid_by_pos_by_layer.keys())\n",
    "    hook_points_cnt = len(hook_points)\n",
    "    for name, module in model.named_modules():\n",
    "        if name in hook_points:\n",
    "            hook_points_cnt -= 1\n",
    "            layer = hook_point_to_layer(name)\n",
    "            for pos, resid in resid_by_pos_by_layer[layer].items():\n",
    "                hook_fn = partial(general_patching_hook_fn, pos=pos, resid=resid)\n",
    "                hook = module.register_forward_hook(hook_fn)\n",
    "                hooks.append(hook)\n",
    "    assert hook_points_cnt == 0\n",
    "    try:\n",
    "        # add and then drop batch dim\n",
    "        logits = model(torch.tensor([input_ids]).cuda()).logits[0]\n",
    "    finally:\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_caching_hook_fn(\n",
    "    module,\n",
    "    input,\n",
    "    output,\n",
    "    pos: slice | int,\n",
    "    resid_by_pos: dict[slice | int, Float[torch.Tensor, \" _seq model\"]],\n",
    "):\n",
    "    # output is 2 el. tuple\n",
    "    output = output[0]\n",
    "    # we're running batch size 1\n",
    "    output = output[0]\n",
    "    # shape is (seq, model)\n",
    "    resid_by_pos[pos] = output[pos].cpu()\n",
    "\n",
    "\n",
    "def clean_run_with_cache(\n",
    "    model: PreTrainedModel,\n",
    "    input_ids: list[int],\n",
    "    pos_by_layer: dict[int, list[slice | int]],\n",
    ") -> tuple[\n",
    "    Float[torch.Tensor, \" seq vocab\"],\n",
    "    dict[int, dict[slice | int, Float[torch.Tensor, \" _seq model\"]]],\n",
    "]:\n",
    "    resid_by_pos_by_layer = {}\n",
    "    hooks = []\n",
    "    hook_points = set(layer_to_hook_point(i) for i in pos_by_layer.keys())\n",
    "    hook_points_cnt = len(hook_points)\n",
    "    for name, module in model.named_modules():\n",
    "        if name in hook_points:\n",
    "            hook_points_cnt -= 1\n",
    "            layer = hook_point_to_layer(name)\n",
    "            assert layer not in resid_by_pos_by_layer\n",
    "            resid_by_pos = resid_by_pos_by_layer[layer] = {}\n",
    "            for pos in pos_by_layer[layer]:\n",
    "                hook_fn = partial(\n",
    "                    general_caching_hook_fn, pos=pos, resid_by_pos=resid_by_pos\n",
    "                )\n",
    "                hook = module.register_forward_hook(hook_fn)\n",
    "                hooks.append(hook)\n",
    "    assert hook_points_cnt == 0\n",
    "    try:\n",
    "        # add and then drop batch dim\n",
    "        logits = model(torch.tensor([input_ids]).cuda()).logits[0]\n",
    "    finally:\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "    return logits, resid_by_pos_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa00cd78c3d24ae29706325d84357701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"hugging-quants/Meta-Llama-3.1-8B-BNB-NF4-BF16\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&lt;|begin_of_text|&gt;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>Today</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;is</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;a</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;good</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;day</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>,</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;I</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;think</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;I</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>'ll</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&lt;|begin_of_text|&gt;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>Today</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;is</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;a</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;bad</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;day</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>,</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;I</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;think</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;I</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>'ll</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt1 = \"Today is a good day, I think I'll\"\n",
    "prompt2 = \"Today is a bad day, I think I'll\"\n",
    "input_ids1 = tokenizer.encode(prompt1)\n",
    "input_ids2 = tokenizer.encode(prompt2)\n",
    "from cot_probing.vis import visualize_tokens_html\n",
    "\n",
    "display(visualize_tokens_html(input_ids1, tokenizer))\n",
    "display(visualize_tokens_html(input_ids2, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_by_layer = {l: [slice(None)] for l in [0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 128256])\n",
      "torch.Size([11, 128256])\n"
     ]
    }
   ],
   "source": [
    "logits1, resid_by_pos_by_layer1 = clean_run_with_cache(model, input_ids1, pos_by_layer)\n",
    "logits2, resid_by_pos_by_layer2 = clean_run_with_cache(model, input_ids2, pos_by_layer)\n",
    "# print(resid_by_pos_by_layer1[10][4].shape)\n",
    "# print(resid_by_pos_by_layer2[10][4].shape)\n",
    "print(logits1.shape)\n",
    "print(logits2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 128256])\n",
      "torch.Size([11, 128256])\n"
     ]
    }
   ],
   "source": [
    "logits_patched_1_to_2 = patched_run(model, input_ids2, resid_by_pos_by_layer1)\n",
    "logits_patched_2_to_1 = patched_run(model, input_ids1, resid_by_pos_by_layer2)\n",
    "print(logits_patched_1_to_2.shape)\n",
    "print(logits_patched_2_to_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq 0 allclose: True\n",
      "seq 1 allclose: True\n",
      "seq 2 allclose: True\n",
      "seq 3 allclose: True\n",
      "seq 4 allclose: True\n",
      "seq 5 allclose: True\n",
      "seq 6 allclose: True\n",
      "seq 7 allclose: True\n",
      "seq 8 allclose: True\n",
      "seq 9 allclose: True\n",
      "seq 10 allclose: True\n",
      "seq 0 allclose: True\n",
      "seq 1 allclose: True\n",
      "seq 2 allclose: True\n",
      "seq 3 allclose: True\n",
      "seq 4 allclose: True\n",
      "seq 5 allclose: True\n",
      "seq 6 allclose: True\n",
      "seq 7 allclose: True\n",
      "seq 8 allclose: True\n",
      "seq 9 allclose: True\n",
      "seq 10 allclose: True\n"
     ]
    }
   ],
   "source": [
    "for seq in range(logits_patched_1_to_2.shape[0]):\n",
    "    allclose = torch.allclose(logits_patched_1_to_2[seq], logits2[seq])\n",
    "    print(f\"seq {seq} allclose: {allclose}\")\n",
    "for seq in range(logits_patched_2_to_1.shape[0]):\n",
    "    allclose = torch.allclose(logits_patched_2_to_1[seq], logits1[seq])\n",
    "    print(f\"seq {seq} allclose: {allclose}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;go</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;celebrate</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;take</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;be</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;make</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;go</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;just</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;stay</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;have</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;be</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topk_toks_1 = logits1[-1].topk(5).indices.tolist()\n",
    "topk_toks_2 = logits2[-1].topk(5).indices.tolist()\n",
    "display(visualize_tokens_html(topk_toks_1, tokenizer))\n",
    "display(visualize_tokens_html(topk_toks_2, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;go</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;just</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;stay</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;have</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;be</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;go</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;celebrate</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;take</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;be</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 255, 255); margin: 0 0 2px -1px; padding: 0'>&nbsp;make</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_toks_patched_1_to_2 = logits_patched_1_to_2[-1].topk(5).indices.tolist()\n",
    "top_toks_patched_2_to_1 = logits_patched_2_to_1[-1].topk(5).indices.tolist()\n",
    "display(visualize_tokens_html(top_toks_patched_1_to_2, tokenizer))\n",
    "display(visualize_tokens_html(top_toks_patched_2_to_1, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
