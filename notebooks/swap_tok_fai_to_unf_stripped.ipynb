{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot_probing.typing import *\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from beartype import beartype\n",
    "import tqdm\n",
    "from cot_probing.generation import categorize_response\n",
    "from cot_probing.diverse_combinations import generate_all_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5bcc50caa64bf983f4a07e363da541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"hugging-quants/Meta-Llama-3.1-70B-BNB-NF4-BF16\"\n",
    "# model_id = \"hugging-quants/Meta-Llama-3.1-8B-BNB-NF4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([42, 13, 21, 51, 76])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"responses_by_seed.pkl\", \"rb\") as f:\n",
    "    responses_by_seed = pickle.load(f)\n",
    "responses_by_seed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['unb', 'bias_no'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 13\n",
    "Q_IDX = 3\n",
    "responses_by_q = responses_by_seed[SEED]\n",
    "responses = responses_by_q[Q_IDX]\n",
    "responses.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_prompts = generate_all_combinations(seed=SEED)[Q_IDX]\n",
    "unbiased_prompt = combined_prompts[\"unb_yes\"]\n",
    "bias_no_prompt = combined_prompts[\"no_yes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unf_resp = responses[\"bias_no\"][\"no\"][0][:-2]\n",
    "fai_resp = responses[\"unb\"][\"yes\"][0][:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get logits on unfaithful CoT in biased and unbiased contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_prompt_tok = tokenizer.encode(unbiased_prompt)\n",
    "bias_no_prompt_tok = tokenizer.encode(bias_no_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 128256])\n",
      "torch.Size([35, 128256])\n"
     ]
    }
   ],
   "source": [
    "def get_logits(prompt_toks: list[int], q_toks: list[int]) -> torch.Tensor:\n",
    "    with torch.inference_mode():\n",
    "        tok_tensor = torch.tensor(prompt_toks + q_toks).unsqueeze(0).to(\"cuda\")\n",
    "        logits = model(tok_tensor).logits\n",
    "        return logits[0, len(prompt_toks) - 1 : -1]\n",
    "\n",
    "\n",
    "unbiased_logits = get_logits(unbiased_prompt_tok, fai_resp)\n",
    "bias_no_logits = get_logits(bias_no_prompt_tok, fai_resp)\n",
    "print(unbiased_logits.shape)\n",
    "print(bias_no_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_other = torch.softmax(bias_no_logits, dim=-1)\n",
    "probs_original = torch.softmax(unbiased_logits, dim=-1)\n",
    "probs_other_orig_diff = probs_other - probs_original\n",
    "max_probs_other_orig_diff = probs_other_orig_diff.max(dim=-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 236, 236); margin: 0 0 2px -1px; padding: 0' title='0.01'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 253, 253); margin: 0 0 2px -1px; padding: 0' title='0.00'>17</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 237, 237); margin: 0 0 2px -1px; padding: 0' title='0.01'>.</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 234, 234); margin: 0 0 2px -1px; padding: 0' title='0.01'>5</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 242, 242); margin: 0 0 2px -1px; padding: 0' title='0.01'>%</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 244, 244); margin: 0 0 2px -1px; padding: 0' title='0.00'>&nbsp;of</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 245, 245); margin: 0 0 2px -1px; padding: 0' title='0.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 251, 251); margin: 0 0 2px -1px; padding: 0' title='0.00'>120</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 173, 173); margin: 0 0 2px -1px; padding: 0' title='0.03'>&nbsp;is</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 242, 242); margin: 0 0 2px -1px; padding: 0' title='0.01'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 128, 128); margin: 0 0 2px -1px; padding: 0' title='0.05'>21</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 76, 76); margin: 0 0 2px -1px; padding: 0' title='0.07'>\\n</div><br><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 226, 226); margin: 0 0 2px -1px; padding: 0' title='0.01'>-</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 232, 232); margin: 0 0 2px -1px; padding: 0' title='0.01'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 252, 252); margin: 0 0 2px -1px; padding: 0' title='0.00'>22</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 245, 245); margin: 0 0 2px -1px; padding: 0' title='0.00'>.</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 234, 234); margin: 0 0 2px -1px; padding: 0' title='0.01'>5</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 235, 235); margin: 0 0 2px -1px; padding: 0' title='0.01'>%</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 240, 240); margin: 0 0 2px -1px; padding: 0' title='0.01'>&nbsp;of</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 242, 242); margin: 0 0 2px -1px; padding: 0' title='0.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 252, 252); margin: 0 0 2px -1px; padding: 0' title='0.00'>80</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 243, 243); margin: 0 0 2px -1px; padding: 0' title='0.00'>&nbsp;is</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 245, 245); margin: 0 0 2px -1px; padding: 0' title='0.00'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 239, 239); margin: 0 0 2px -1px; padding: 0' title='0.01'>18</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 239, 239); margin: 0 0 2px -1px; padding: 0' title='0.01'>\\n</div><br><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 235, 235); margin: 0 0 2px -1px; padding: 0' title='0.01'>-</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 234, 234); margin: 0 0 2px -1px; padding: 0' title='0.01'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 237, 237); margin: 0 0 2px -1px; padding: 0' title='0.01'>21</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 206, 206); margin: 0 0 2px -1px; padding: 0' title='0.02'>&nbsp;+</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 238, 238); margin: 0 0 2px -1px; padding: 0' title='0.01'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 252, 252); margin: 0 0 2px -1px; padding: 0' title='0.00'>18</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 0, 0); margin: 0 0 2px -1px; padding: 0' title='0.10'>&nbsp;is</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 102, 102); margin: 0 0 2px -1px; padding: 0' title='0.06'>&nbsp;</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 254, 254); margin: 0 0 2px -1px; padding: 0' title='0.00'>39</div><div style='display: inline-block; border: 1px solid #888; font-family: monospace; font-size: 14px; color: black; background-color: rgb(255, 241, 241); margin: 0 0 2px -1px; padding: 0' title='0.01'>\\n</div><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cot_probing.vis import visualize_tokens_html\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    visualize_tokens_html(\n",
    "        fai_resp,\n",
    "        tokenizer,\n",
    "        token_values=max_probs_other_orig_diff.tolist(),\n",
    "        vmin=0.0,\n",
    "        vmax=max_probs_other_orig_diff.max().item(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cot_probing.swapping import try_swap_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 2 other tokens\n",
      "Trying to swap original CoT token ` is`\n",
      "Original CoT token and other token are the same, skipping...\n",
      "Trying to swap original CoT token ` is`\n",
      "Swapping with other token ` does`\n",
      "truncated cot:\n",
      " 17.5% of 120 is 21\n",
      "- 22.5% of 80 is 18\n",
      "- 21 + 18\n",
      "###\n",
      "original answer: yes\n",
      "` 39\n",
      "Answer:`\n",
      "###\n",
      "swapped answer: no\n",
      "` not equal 39\n",
      "Answer:`\n",
      "Trying 1 other tokens\n",
      "Trying to swap original CoT token `\n",
      "`\n",
      "Original CoT token and other token are the same, skipping...\n",
      "Trying 1 other tokens\n",
      "Trying to swap original CoT token ` `\n",
      "Swapping with other token ` not`\n",
      "truncated cot:\n",
      " 17.5% of 120 is 21\n",
      "- 22.5% of 80 is 18\n",
      "- 21 + 18 is\n",
      "###\n",
      "original answer: yes\n",
      "`39\n",
      "Answer:`\n",
      "###\n",
      "swapped answer: no\n",
      "` equal to 39\n",
      "Answer:`\n",
      "Trying 1 other tokens\n",
      "Trying to swap original CoT token `21`\n",
      "Original CoT token and other token are the same, skipping...\n",
      "Trying 1 other tokens\n",
      "Trying to swap original CoT token ` is`\n",
      "Original CoT token and other token are the same, skipping...\n",
      "Best swap token:  does\n",
      "Best prob diff: 0.0651\n",
      "Best seq pos: 31\n"
     ]
    }
   ],
   "source": [
    "topk_metric_indices = max_probs_other_orig_diff.topk(k=5).indices.tolist()\n",
    "best_prob_diff = 0.0\n",
    "best_swap_tok = None\n",
    "best_seq_pos = None\n",
    "for seq_pos in topk_metric_indices:\n",
    "    swap_result = try_swap_position(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        original_ctx_toks=unbiased_prompt_tok,\n",
    "        unbiased_ctx_toks=unbiased_prompt_tok,\n",
    "        original_cot=fai_resp,\n",
    "        original_expected_answer=\"yes\",\n",
    "        probs_other_orig_diff=probs_other_orig_diff,\n",
    "        seq_pos=seq_pos,\n",
    "    )\n",
    "    if swap_result is None:\n",
    "        continue\n",
    "    swap_tok, prob_diff = swap_result\n",
    "    if prob_diff > best_prob_diff:\n",
    "        best_prob_diff = prob_diff\n",
    "        best_swap_tok = swap_tok\n",
    "        best_seq_pos = seq_pos\n",
    "print(f\"Best swap token: `{tokenizer.decode([best_swap_tok])}`\")\n",
    "print(f\"Best prob diff: {best_prob_diff:.4f}\")\n",
    "print(f\"Best seq pos: {best_seq_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
